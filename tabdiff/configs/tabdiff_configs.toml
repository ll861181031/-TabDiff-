[data]
dequant_dist = "none"
int_dequant_factor = 0

[unimodmlp_params]
num_layers = 4
d_token = 8
n_head = 2
factor = 64
bias = true
dim_t = 1024
use_mlp = true

[diffusion_params]
num_timesteps = 100
scheduler='power_mean_per_column'      # 'power_mean', 'power_mean_unified', 'power_mean_per_column'
cat_scheduler='log_linear_per_column'        # 'log_linear', 'log_linear_unified', 'log_linear_per_column'
noise_dist='uniform_t'       #'uniform_t' or 'log_norm'

[diffusion_params.sampler_params]
stochastic_sampler = true
second_order_correction = true

[diffusion_params.edm_params]
precond = true
sigma_data = 1.0
net_conditioning = "sigma"

[diffusion_params.noise_dist_params]
P_mean = -1.0
P_std = 1.5

[diffusion_params.noise_schedule_params]
sigma_min = 0.002
sigma_max = 100
rho = 7
eps_max = 1e-3
eps_min = 1e-5
rho_init = 7.0
rho_offset = 5.0
k_init=-6.0
k_offset=1.0

[train.main]
steps = 1200
lr = 0.0005
weight_decay = 1e-5
ema_decay = 0.999
batch_size = 64
check_val_every = 200
lr_scheduler = "reduce_lr_on_plateau"
factor = 0.90           # hyperparam for reduce_lr_on_plateau
reduce_lr_patience = 30        # hyperparam for reduce_lr_on_plateau
closs_weight_schedule = "anneal"
c_lambda = 1.0
d_lambda = 1.0

[sample]
batch_size = 10000
